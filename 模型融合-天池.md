---
title: 模型融合
categories: Python机器学习
---

# 集成学习/模型融合

按照个体学习器的关系，模型融合提升技术可以分为两类:  
(1)个体学习器间不存在强依赖关系可同时生成的并行化方法，代表是Bagging 方法和随机森林。  
(2)个体学习器间存在强依赖关系必须串行生成的序列化方法，代表是Boosting方法。  

集成学习（ensemble learning）通过构建并结合多个学习器来完成学习任务。

如何产生“好而不同”的个体学习器，是集成学习研究的核心。

集成学习的思路是通过合并多个模型来提升机器学习性能，这种方法相较于当个单个模型通常能够获得更好的预测结果。一般来说集成学习可以分为三大类：

- 用于减少方差的bagging
- 用于减少偏差的boosting
- 用于提升预测结果的stacking

参考：[偏差（Bias）与方差（Variance）](https://zhuanlan.zhihu.com/p/38853908)

## Bagging装袋（并行）

Bagging就是采用有放回的方式进行抽样，用抽样的样本建立子模型,对子模型进行训练，这个过程重复多次，最后进行融合。

Bagging方法采用的是自助采样法( Bootstrap sampling),即对于m个样本的原始训练集，每次先随机采集一个样本放入采样集，接着把该样本放回，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集。由于是随机采样，因此每次的采样集和原始的训练集不同，和其他采样集也不同，这样就可以得到多个不同的弱学习器。

通常分类任务使用投票的方式集成，而回归任务通过平均的方式集成。

投票法（Voting）:如果是分类模型，每个模型都会给出一个类别预测结果，通过投票的方式，按照少数服从多数的原则融合得到一个新的预测结果。
均值法（Averaging）:如果是回归模型，每个模型给出的预测结果都是数值型的，这时候我们可以通过求所有子模型的预测结果的均值作为最终的融合结果。

Bagging + 决策树=随机森林

## boosting提高（串行）

Boosting是一种将各种弱分类器串联起来的集成学习方式，每一个分类器的训练都依赖于前一个分类器的结果，顺序运行的方式导致了运行速度慢。和所有融合方式一样，它不会考虑各个弱分类器模型本身结构为何，而是对训练数据（样本集）和连接方式进行操纵以获得更小的误差。但是为了将最终的强分类器的误差均衡，之前所选取的分类器一般都是相对比较弱的分类器，因为一旦某个分类器较强将使得后续结果受到影响太大。所以多用于集成学习而非模型融合（将多个已经有较好效果的模型融合成更好的模型）。

Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2；如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

Boosting融合在每次训练模型时更关注上一次的模型错判的样例，并且会给这些错判的样例更大的权重，这样做的目的是就是为了加强对错判样本的学习，让模型通过不断的迭代，效果越来越好。最终将多次迭代的训练得到的弱模型进行加权求和，得到最终的强模型。因为Boosting框架各模型间是有依赖关系存在的，所以它是一种串行的融合方法。

常见的boosting方法有，Adaboost、GBDT、XGBOOST等。

## Bagging，Boosting二者之间的区别

1、样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2、样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3、预测函数：

Bagging：所有预测函数的权重相等。

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4、并行计算：

Bagging：各个预测函数可以并行生成，即个体学习器间不存在强依赖关系，可同时生成的并行化方法。

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。即个体学习器间存在强依赖关系，不可同时生成的序列化方法。

5、bagging减少variance，而boosting是减少bias

Bagging对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均。由于子样本集的相似性以及使用的是同种模型，因此各模型有近似相等的bias和variance（事实上，各模型的分布也近似相同，但不独立）。另一方面，若各子模型独立，则有，此时可以显著降低variance。若各子模型完全相同，则此时不会降低variance。bagging方法得到的各子模型是有一定相关性的，属于上面两个极端状况的中间态，因此可以一定程度降低variance。

boosting是在sequential地最小化损失函数，其bias自然逐步下降。但由于是采取这种sequential、adaptive的策略，各子模型之间是强相关的，于是子模型之和并不能显著降低variance。所以说boosting主要还是靠降低bias来提升预测精度。

## stacking堆叠

Stacking方法是一种分层模型集成框架。以两层为例，首先将数据集分成训练集和测试集，利用训练集训练得到多个初级学习器，然后用初级学习器对测试集进行预测，并将输出值作为下一阶段训练的输入值，最终的标签作为输出值，用于训练次级学习器（通常最后一级使用Logistic回归）。由于两次所使用的训练数据不同，因此可以在一定程度上防止过拟合。

![image-20210413103615325](https://gitee.com/itangqian/picgo/raw/master/img/image-20210413103615325.png)

假设我们有5个基模型

1、首先我们将训练集分为五份。

2、对于每一个基模型来说，我们用其中的四份来训练，对未用来的训练的一份训练集（验证集）进行预测，将预测值纵向合并形成特征1，然后对测试集进行预测，得到预测值。然后改变所选的用来训练的训练集和用来验证的训练集，重复此步骤，直到获得完整的训练集的预测结果（预测值为5次的平均值）。

3、对五个模型，分别进行步骤2，我们将获得5个模型，以及五个模型分别通过交叉验证获得的训练集预测结果。即P1、P2、P3、P4、P5，即5个特征，横向拼接。

4、用五个模型分别对测试集进行预测，得到测试集的预测结果：T1、T2、T3、T4、T5，也是5个特征，横向拼接。

5、将P1~5、T1~5作为下一层的训练集和测试集。（当然，target还是之前的target）

```python
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import StackingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)


rfc = RandomForestClassifier(n_estimators=100, random_state=0)
gbc = GradientBoostingClassifier(n_estimators=100, random_state=0)

# 用 StackingClassifier 作为元估计器（meta-estimators），来集成两个子估计器（base-estimator）
# 随机森林分类器 rfc 和梯度提升分类器 gbc 作为一级分类器，之后用逻辑回归分类器作为二级分类器。
estimators = [('rf', rfc), ('gb', gbc)]
clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())

# 比较子估计器和元估计器的在测试集上的表现
rfc.fit(X_train, y_train)
gbc.fit(X_train, y_train)
clf.fit(X_train, y_train)
print(rfc.score(X_test, y_test))
print(gbc.score(X_test, y_test))
print(clf.score(X_test, y_test))
```

```
0.9649122807017544
0.9649122807017544
0.9736842105263158
```

参考: [集成学习--bagging、boosting、stacking](https://blog.csdn.net/zwqjoy/article/details/80431496)

[模型融合Bagging，Boosting，Stacking](https://blog.csdn.net/xiaohutong1991/article/details/107976781)

[集成学习介绍之三——Stacking算法](https://zhuanlan.zhihu.com/p/160809784)

