# 特征工程重要性和处理

特征工程就是从原始数据提取特征的过程。

特征工程一般包括特征使用、特征获取、特征处理、特征选择和特征监控。

## 重要性

数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。

特征越好，灵活性越强，构建的模型越简单、性能越出色。

## 处理过程

- 去掉无用特征
- 去除冗余特征，如共线特征
- 生成新特征
- 转换特征，如数值化、类别转换、归一化等
- 对特征进行处理，如异常值、最大值、最小值、缺失值等

简单来说，特征工程的处理一般包括数据预处理、特征处理、特征选择等工作，而特征选择视情况而定，如果特征数量较多，则可以进行特征选择等操作。

# 数据预处理和特征处理

## 数据预处理

数据采集、数据清洗、数据采样。

### 数据清洗

- 简单属性判定：如身高3米多的人，一个人每月购买10万元的美发卡。
- 组合或统计属性判定：如号称在美国却一直都是国内的新闻阅读用户。
- 补齐可对应的缺省值:将不可信的样本丢掉，不用缺省值极多的字段。

### 数据采样

- 随机采样，一般不采用，因为可能某次随机采样得到的数据很不均匀
- 分层采样
  - 正样本>负样本，且量都特别大的情况:采用下采样(downsampling) 的方法。
  - 正样本>负样本，且量不大的情况，可采用以下方法采集更多的数据:上采样(oversampling),比如图像识别中的镜像和旋转:修改损失函数(loss function)设置样本权重。

## 特征处理

标准化、区间缩放法、归一化、定量特征二值化、定性特征哑编码、缺失值处理和数据转换。

### 标准化

对列进行处理，将数据转换为标准正态分布。

$$x^{*}=\frac{x-\mu}{\sigma}$$

$\mu$ 为期望,均值
$\sigma$ 为方差

```python
from sklearn.preprocessing import StandardScaler
StandardScaler().fit_transform(data)
```

### 区间缩放法

区间缩放的思路有很多种，常见的一种是利用最大值和最小值进行缩放
$$
x^{\prime}=\frac{x-M i n}{M a x-M i n}
$$

```python
from sklearn.preprocessing import MinMaxScaler
#区间缩放，返回值为缩放到[0, 1]区间的数据
MinMaxScaler().fit_transform(data)
```

### 归一化/正则化

归一化是按照数据的**行**进行处理的.该方法主要应用于文本分类和聚类中.计算两个样本之间的相似性.

归一化是将样本的特征值转换到同一量纲下，把数据映射到[0,1]或者[a,b]区间内，由于其仅由变量的极值决定，因此区间缩放法是归一化的一种。

归一化会改变数据的原始距离、分布和信息，但标准化一般不会。

正则化的过程是将每个样本缩放到单位范数.

0范数，向量中非零元素的个数。

1范数，为绝对值之和。

2范数，就是通常意义上的模。

无穷范数，就是取向量的最大值。

- 向量的范数
  - 1-范数：$\|X\|_{1}=\sum_{i=1}^{n}\left|x_{i}\right|$，各个元素的绝对值之和
  - 2-范数：$\|X\|_{2}=\left(\sum_{i=1}^{n} x_{i}^{2}\right)^{\frac{1}{2}}=\sqrt{\sum_{i=1}^{n} x_{i}^{2}}$，每个元素的平方和再开平方根
  - p-范数：$\|X\|_{p}=\left(\sum_{i=1}^{n}\left|x_{i}\right|^{p}\right)^{\frac{1}{p}}$
  - 无穷范数：$\|X\|_{\infty}=\max _{1 \leq i \leq n}\left|x_{i}\right|$，向量的所有元素的绝对值中**最大**的
  

规则为L2的归一化公式为:$${ x }^{ ' }=\frac { x }{ \sqrt { \sum _{ i=1 }^{ n } x_{ i }^{ 2 } }  } $$

```python
from sklearn.preprocessing import Normalizer

Normalizer().fit_transform(data)
```

归一化与标准化的使用场景:

- 如果对输出结果范围有要求， 则用归一化。
- 如果数据较为稳定， 不存在极端的最大值或最小值，则用归一化。
- 如果数据存 在异常值和较多噪声，则用标准化，这样可以通过中心化间接避免异常值和极端值的影响。
- 支持向量机 (Support Vector Machine, SVM)、K近邻(K-Nearest Neighbor, KNN)、主成分分析(Principal Component Analysis, PCA)等模型都必须进行归一化或标准化操作。

### 定量特征二值化

核心在于设定一个阈值,大于阈值的赋值为1,小于等于阈值的赋值为0

$$
x^{\prime}=\left\{\begin{array}{l}1, x>\text { threshold } \\ 0, x \leq \text { threshold }\end{array}\right.
$$

```python
from sklearn.preprocessing import Binarizer

Binarizer(threshold=3).fit_transform(data)
```

### 对定性特征哑编码

哑编码:OneHotEncoder, 直观来说就是有多少个状态就有多少比特，而且只有一个比特为1，其他全为0的一种码制。

参考:https://blog.csdn.net/weixin_40807247/article/details/82812206

```python
from sklearn.preprocessing import OneHotEncoder
#哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据
OneHotEncoder().fit_transform(iris.target.reshape((-1,1))).toarray()
```

### 缺失值处理

```python
from numpy import vstack, array, nan
from sklearn.impute import SimpleImputer
import pandas as pd

data = pd.DataFrame([[7, 2, 3], [4, np.nan, 6], [10, 5, 9]])
SimpleImputer().fit_transform(data)
```

### 数据转换

多项式转换、指数转换、对数转换

#### 多项式转化

例如4个特征，度为2的多项式转换会转换为15个特征，度为2的意思是要生成最多两个原始特征的乘积组成的所有特征。
$$1,x_{1}, x_{2}, x_{3}, x_{4}$$，1为常数特征
$$x_{1}^{2}, x_{1} * x_{2}, x_{1} * x_{3}, x_{1} * x_{4}$$
$$x_{2}^{2}, x_{2} * x_{3}, x_{2} * x_{4}$$
$$x_{3}^{2}, x_{3} * x_{4}, x_{4}^{2}$$

```python
from sklearn.preprocessing import PolynomialFeatures

PolynomialFeatures().fit_transform(iris.data).shape
```

#### 对数转换

```python
from sklearn.preprocessing import FunctionTransformer
from numpy import log1p

FunctionTransformer(log1p, validate=False).fit_transform(iris.data)[:5]
```

#### 特征处理小结

| 类                  | 功能               | 说明                                                         |
| ------------------- | ------------------ | ------------------------------------------------------------ |
| StandardScaler      | 无量纲化           | 标准化，基于特征矩阵的**列**，将特征值转换为服从**标准正态分布** |
| MinMaxScaler        | 无量纲化           | 区间缩放，基于最大值或最小值，将特征值转换到[0, 1]区间内     |
| Normalizer          | 归一化             | 基于特征矩阵的**行**，将样本向量转换为单位向量               |
| Binarizer           | 定量特征二值化     | 基于给定阈值，将定量特征按阈值划分                           |
| OneHotEncoder       | 定性特征哑编码     | 将定性特征编码为定量特征                                     |
| Imputer             | 缺失值处理         | 计算缺失值，缺失值可填充为均值等                             |
| PolynomialFeatures  | 多项式数据转换     | 多项式数据转换                                               |
| FunctionTransformer | 自定义单元数据转换 | 使用单变元函数转换数据                                       |

# 特征降维

特征选择和线性降维。

## 特征选择

删除不必要的特征。

| 特征选择的目标     | 说明                                                         |
| ------------------ | ------------------------------------------------------------ |
| 特征是否发散       | 如果一个特征不发散，如方差接近于0,就说明样本在这个特征上基本没有差异，这个特征对于样本的区分没有作用 |
| 特征与目标的相关性 | 与目标相关性高的特征应当优选选择                             |

## 特征选择的方法

特征选择的方法有过滤法(Fiter)、 包装法(Wrapper) 和嵌入法(Embedded)。

- 过滤法:按照发散性或者相关性对各个特征进行评分，通过设定阈值或者待选择阈值的个数来选择特征。
  - 思路：特征变量和目标变量之间的关系
  - 相关系数
  - 卡方检验
  - 信息增益、互信息
- 包装法:根据目标函数(通常是预测效果评分)每次选择若干特征，或者排除若干特征。
  - 思路：通过目标函数（AUC/MSE等）来决定是否加入一个变量
  - 迭代：产生特征子集，评价
    - 完全搜索
    - 启发搜索
    - 随机搜索
      - 遗传算法GA
      - 模拟退火算法SA
- 嵌入法:使用机器学习的某些算法和模型进行训练，得到各个特征的权值系数，并根据系数从大到小选择特征。这方法类似于过滤法，区别在于它通过训练来确定特征的优劣。
  - 思路：学习期自身自动选择特征
  - 正则化
    - L1：LASSO
    - L2：ridge
  - 决策树：熵、信息增益
  - 深度学习

## sklearn

| 类                | 所属方法 | 具体方法                                                 |
| ----------------- | -------- | -------------------------------------------------------- |
| VarianceThreshold | Filter   | 方差选择法                                               |
| SelectKBest       | Filter   | 将可选相关系数、卞方检验或最大信息系数作カ得分计算的方法 |
| RFE               | Wrapper  | 递归消除特征法                                           |
| SelectFromModel   | Embedded | 基于模型的特征选择法                                     |

### 方差选择法VarianceThreshold

先要计算各个特征的方差，然后根据阈值选择方差大于阈值的特征。

```python
from sklearn.feature_selection import VarianceThreshold

VarianceThreshold(threshold=3).fit_transform(iris.data)[:5]
```

### SelectKBest

#### 相关系数法

先要计算各个特征对目标值的相关系数及相关系数的P值，然后根据阈值筛选特征。

```python
from sklearn.feature_selection import SelectKBest
from scipy.stats import pearsonr

SelectKBest(lambda X, Y: np.array(list(map(lambda x: pearsonr(x, Y), X.T))).T[0], k=2).fit_transform(iris.data, iris.target)[:5]
```

#### 卡方检验

检验定性自变量与定性因变量的相关性。

```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)[:5]
```

#### 最大信息系数法

评价定性自变量与定性因变量的相关性。

```python
from sklearn.feature_selection import SelectKBest
from minepy import MINE

def mic(x, y):
    m = MINE()
    m.compute_score(x, y)
    return (m.mic(), 0.5)

SelectKBest(lambda X, Y: np.array(list(map(lambda x: mic(x, Y), X.T))).T[0], k=2).fit_transform(iris.data, iris.target)[:5]
```

### RFE

递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

RFE(estimator=LogisticRegression(max_iter=500), n_features_to_select=2).fit_transform(iris.data, iris.target)[:5]
```

### SelectFromModel(TODO)